---
title: ""
layout: home
permalink: /
classes: wide
redirect_from:
  - /portfolio/
  - /portfolio
---

## About Me

I'm a Research Engineer at the [Stanford Vision & Learning Lab](https://svl.stanford.edu/), where I pursue my passion for robotics and machine learning. My research focuses on robotics simulation and data curation for robot learning.

I received my M.S. in Robotics and B.S. in Computer Science from Northwestern University. I was also fortunate enough to have the opportunity to work on the [OTTAVA](https://thenext.jnjmedtech.com/surgical-robotics) surgical robotic system as part of the Robotics & Controls team at Johnson & Johnson MedTech.

## Professional Experience
<table>
  <tbody>
    <tr>
      <td style = "border-bottom-width:0;"><img src="{{site.baseurl}}/assets/images/sail.png" alt="stanford" width="60"></td>
      <td style = "border-bottom-width:0;">
        <strong>Stanford AI Lab</strong> <br> 01/2024 - present <br> Research Engineer</td>
      <td style = "border-bottom-width:0;"><img src="{{site.baseurl}}/assets/images/johnson-and-johnson.png" alt="j&j" width="60"></td>
      <td style = "border-bottom-width:0;">
        <strong>Johnson & Johnson</strong> <br> 06/2023 - 09/2023 <br> Robotics&Controls Intern</td>
      <td style="border-bottom-width:0;"><img src="{{site.baseurl}}/assets/images/delta-lab.png" alt="nu" width="60"></td>
      <td style="border-bottom-width:0;">
        <strong>Northwestern Delta Lab</strong> <br> 03/2021 - 06/2022 <br> Research Assistant</td>
    </tr>
  </tbody>
</table>

## Education
<table>
  <tbody>
    <tr>
      <td style="border-bottom-width:0;"><img src="{{site.baseurl}}/assets/images/northwestern.jpg" alt="nu" width="60"></td>
      <td style="border-bottom-width:0;">
        <strong>Northwestern University</strong> <br> 09/2022 - 12/2023 <br> M.S. in Robotics
      </td>
      <td style="border-bottom-width:0;"><img src="{{site.baseurl}}/assets/images/northwestern.jpg" alt="nu" width="60"></td>
      <td style="border-bottom-width:0;">
        <strong>Northwestern University</strong> <br> 09/2019 - 06/2022 <br> B.S. with honors in Computer Science, <em>summa cum laude</em>
      </td>
    </tr>
  </tbody>
</table>


## Publications

<div class="container">
  <div class="image-container">
    <a href="https://behavior-robot-suite.github.io/">
      <img src="{{site.baseurl}}/assets/images/brs_hardware.jpg" alt="brs">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="https://behavior-robot-suite.github.io/" class="title-link">
        <h3>BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities</h3>
      </a>
    </div>
    <div class="text-content">
      <p>We introduce the BEHAVIOR Robot Suite (BRS) for household mobile manipulation, featuring a bimanual wheeled robot with 4-DoF torso that achieves critical capabilities in coordination, navigation, and reachability. Our framework includes a cost-effective teleoperation interface and novel algorithm for learning whole-body visuomotor policies.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="https://behavior.stanford.edu/">
      <img src="{{site.baseurl}}/assets/images/b1k.jpg" alt="b1k">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="https://behavior.stanford.edu/" class="title-link">
        <h3>BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation</h3>
      </a>
    </div>
    <div class="text-content">
      <p>BEHAVIOR-1K is a comprehensive simulation benchmark for human-centered robotics with 1,000 real-world tasks. Powered by NVIDIA's Omniverse, it features diverse scenes, objects, and activities with realistic rendering and physics simulation. This benchmark aims to advance embodied AI and robot learning research.</p>
    </div>
  </div>
</div>

## Past Projects

<div class="container">
  <div class="image-container">
    <a href="/portfolio_manipulation/omnids">
      <img src="{{site.baseurl}}/assets/images/omnids.gif" alt="omnids">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_manipulation/omnids" class="title-link">
        <h3>Action Chunking with Transformers (ACT) for Assistive Action Prediction</h3>
      </a>
    </div>
    <div class="text-content">
      <p>This project aims to enhance human-robot collaboration with Omnid Mocobots by implementing imitation learning approaches (Action Chunking with Transformers and Diffusion Policy) to predict human intent during co-manipulation tasks.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_manipulation/robot-chef">
      <img src="{{site.baseurl}}/assets/images/robot_chef.gif" alt="robot-chef">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_manipulation/robot-chef" class="title-link">
        <h3>Autonomous Robot Chef</h3>
      </a>
    </div>
    <div class="text-content">
      <p>This project created a voice-controlled robotic cooking assistant that combines LLM for recipe planning, CLIP for object detection, and MediaPipe with LSTM for hand gesture recognition, allowing users to interact with a robot arm through Alexa voice commands to collaboratively prepare meals.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_perception_nav/slam">
      <img src="{{site.baseurl}}/assets/images/slam_gif.gif" alt="slam">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_perception_nav/slam" class="title-link">
        <h3>Feature-based EKF-SLAM from Scratch</h3>
      </a>
    </div>
    <div class="text-content">
      <p>This project implemented a feature-based EKF-SLAM (Extended Kalman Filter for Simultaneous Localization and Mapping) system from scratch in C++, using ROS2 and Rviz for simulation, with custom landmark detection algorithms to enable a robot to simultaneously map its environment and determine its location within it.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_manipulation/jenga">
      <img src="{{site.baseurl}}/assets/images/jenga.gif" alt="jenga-project">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_manipulation/jenga" class="title-link">
        <h3>Jengabells - Franka Plays Jenga</h3>
      </a>
    </div>
    <div class="text-content">
      <p>This project developed a Jenga-playing assistant using a Franka Emika Panda robotic arm, combining computer vision for tower and brick detection, a custom MoveIt API for motion planning, and transfer learning on MobileNets for hand detection. </p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_ML/nlp-project">
      <img src="{{site.baseurl}}/assets/images/nlp_pipeline.jpg" alt="nlp-project">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_ML/nlp-project" class="title-link">
        <h3>Edge-conditioned Graph Conlution Networks with Dependency Parsing</h3>
      </a>
    </div>
    <div class="text-content">
      <p>A project that combines dependency parsing and pre-trained language models using edge-conditioned graph convolutional networks (GCN) for sentiment analysis, achieving 87.36% accuracy on IMDB binary classification with fewer parameters than traditional methods, while maintaining interpretability by representing sentences as graphs with both semantic and syntactic features.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_manipulation/kuka">
      <img src="{{site.baseurl}}/assets/images/449.gif" alt="kuka">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_manipulation/kuka" class="title-link">
        <h3>KUKA youBot Mobile Manipulation</h3>
      </a>
    </div>
    <div class="text-content">
      <p>A project implementing trajectory planning and control for a KUKA youBot mobile manipulator (featuring a 5R robot arm mounted on a four-wheeled mecanum base), which performs an eight-segment pick-and-place operation using feedforward control and odometry-based kinematics simulation to accurately move blocks between specified locations.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_perception_nav/visual-odom">
      <img src="{{site.baseurl}}/assets/images/visual-odom1.gif" alt="visual-odom">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_perception_nav/visual-odom" class="title-link">
        <h3>RoI-bounded Visual Odometry</h3>
      </a>
    </div>
    <div class="text-content">
      <p>This project optimized monocular visual odometry by implementing a region-of-interest (ROI) based approach for feature detection, significantly reducing computational requirements while maintaining accuracy compared to traditional methods that process entire image frames.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_perception_nav/swarm">
      <img src="{{site.baseurl}}/assets/images/swarm-tn.png" alt="swarm">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_perception_nav/swarm" class="title-link">
        <h3>Swarm Shape and Behavior Control</h3>
      </a>
    </div>
    <div class="text-content">
      <p>This project implemented two swarm control algorithms: one simulating the Brazil nut effect for spatial sorting of robots by size, and another replicating Reynolds' flocking behavior to coordinate robot movement like bird flocks, with both systems using distributed control methods.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_mechatronics/quadrotor">
      <img src="{{site.baseurl}}/assets/images/quadrotor.jpg" alt="quadrotor">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_mechatronics/quadrotor" class="title-link">
        <h3>Quadrotor Design and Control</h3>
      </a>
    </div>
    <div class="text-content">
      <p>This project built an autonomous quadrotor drone by implementing IMU integration, PID control systems, manual joystick control, and Vive lighthouse positioning, enabling both manual and autonomous flight capabilities.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_mechatronics/mario">
      <img src="{{site.baseurl}}/assets/images/mario-luigi.png" alt="mario">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_mechatronics/mario" class="title-link">
        <h3>Mario Motorcycle</h3>
      </a>
    </div>
    <div class="text-content">
      <p>Inspired by Mario Kart, this project involved creating a line-following motorcycle that uses a Raspberry Pi Pico W for image processing and a PIC microcontroller for steering control, complete with custom-designed PCBs, earning "Best Design" at the 2023 Northwestern Tech Cup.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_mechatronics/imu-emg">
      <img src="{{site.baseurl}}/assets/images/imu-emg.jpg" alt="imu-emg">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_mechatronics/imu-emg" class="title-link">
        <h3>IMU & EMG Controlled Robot Arm</h3>
      </a>
    </div>
    <div class="text-content">
      <p>A collaborative project where two teams created the IMUGripulator - a 2-DOF robotic arm system combining IMU-based joint control and EMG-based gripper control, built using micro:bit v2 microcontrollers and programmed in C, featuring various sensors including a 9-DOF IMU for tilt-based movement and capacitive touch controls for sensitivity adjustment.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_mechatronics/dice-cup">
      <img src="{{site.baseurl}}/assets/images/314.gif" alt="dice-cup">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_mechatronics/dice-cup" class="title-link">
        <h3>Dice in a Cup Simulation from Scratch</h3>
      </a>
    </div>
    <div class="text-content">
      <p>A physics simulation project that models a dice bouncing inside a spinning cup, implementing Lagrangian dynamics to handle the 6 degrees of freedom system, including collision detection between the dice's corners and cup edges, while accounting for gravitational and external forces to maintain the cup's position.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_web/os-env">
      <img src="{{site.baseurl}}/assets/images/os-env.jpg" alt="os-env">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_web/os-env" class="title-link">
        <h3>Orchestration Scripting Environment</h3>
      </a>
    </div>
    <div class="text-content">
      <p>This project developed Orchestration Scripts, a framework that supports effective work practices by detecting workplace situations and providing tailored strategies using computational abstractions of organizational processes, structures, venues, and tools.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_ML/iExpressionNet">
      <img src="{{site.baseurl}}/assets/images/emotion_detection.png" alt="emotion-detection">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_ML/iExpressionNet" class="title-link">
        <h3>iExpressionNet</h3>
      </a>
    </div>
    <div class="text-content">
      <p>A deep learning project that improves facial expression detection accuracy for specific users by combining transfer learning with a two-stage approach: first training a CNN on the general FER-13 dataset, then fine-tuning the model using individual users' facial expression data while keeping the convolutional layers frozen, integrated with OpenCV for face detection.</p>
    </div>
  </div>
</div>

<style>
.container {
  display: flex;
  margin-bottom: 10px;
  gap: 10px;
}

.image-container {
  flex: 0 0 200px;
  height: 100px;
  overflow: hidden;
}

.image-container img {
  width: 100%;
  height: 100%;
  object-fit: cover;
  object-position: center;
  display: block;
  transition: opacity 0.2s;
}

/* Special handling for logo-style images in the experience/education tables */
table img {
  width: 60px;
  height: 60px;
  object-fit: contain;
}

.image-container img:hover {
  opacity: 0.8;
}

.text-container {
  flex: 1;
  display: flex;
  flex-direction: column;
  min-height: 100px;
  justify-content: flex-start;
}

.header-row {
  display: flex;
  justify-content: space-between;
  align-items: flex-start;
  margin-bottom: 0.25rem;
}

.header-row h3 {
  margin: 0;
  font-size: 0.8rem;
  color: #333;
  transition: color 0.2s;
  line-height: 1.2;
}

.title-link {
  text-decoration: none;
  color: inherit;
}

.title-link:hover h3 {
  color: #0066cc;
  text-decoration: underline;
}

.text-content p {
  margin: 0;
  font-size: 0.6rem;
  line-height: 1.4;
  color: #666;
}
</style>