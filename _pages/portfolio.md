---
title: ""
layout: home
permalink: /
classes: wide
redirect_from:
  - /portfolio/
  - /portfolio
---

## About Me

I'm a Software Engineer at the [Stanford Vision & Learning Lab](https://svl.stanford.edu/), where I focus on robotics research. My work centers on developing 3D robot simulations and curating datasets to advance robot learning.

## Professional Experience
<table>
  <tbody>
    <tr>
      <td style = "border-bottom-width:0;"><img src="{{site.baseurl}}/assets/images/sail.png" alt="stanford" width="60"></td>
      <td style = "border-bottom-width:0;">
        <strong>Stanford AI Lab</strong> <br> 01/2024 - present <br> Software Developer 2</td>
      <td style = "border-bottom-width:0;"><img src="{{site.baseurl}}/assets/images/johnson-and-johnson.png" alt="j&j" width="60"></td>
      <td style = "border-bottom-width:0;">
        <strong>Johnson & Johnson</strong> <br> 06/2023 - 09/2023 <br> Robotics & Controls</td>
      <td style="border-bottom-width:0;"><img src="{{site.baseurl}}/assets/images/delta-lab.png" alt="nu" width="60"></td>
      <td style="border-bottom-width:0;">
        <strong>Northwestern Delta Lab</strong> <br> 03/2021 - 06/2022 <br> Research Assistant</td>
    </tr>
  </tbody>
</table>

## Education
<table>
  <tbody>
    <tr>
      <td style="border-bottom-width:0;"><img src="{{site.baseurl}}/assets/images/northwestern.jpg" alt="nu" width="60"></td>
      <td style="border-bottom-width:0;">
        <strong>Northwestern University</strong> <br> 09/2022 - 12/2023 <br> M.S. in Robotics
      </td>
      <td style="border-bottom-width:0;"><img src="{{site.baseurl}}/assets/images/northwestern.jpg" alt="nu" width="60"></td>
      <td style="border-bottom-width:0;">
        <strong>Northwestern University</strong> <br> 09/2019 - 06/2022 <br> B.S. with honors in Computer Science, <em>summa cum laude</em>
      </td>
    </tr>
  </tbody>
</table>


## Research

<div class="container">
  <div class="image-container">
    <a href="https://behavior.stanford.edu/challenge/">
      <img src="{{site.baseurl}}/assets/images/behavior_challenge.png" alt="behavior-challenge">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="https://behavior.stanford.edu/challenge/" class="title-link">
        <h3>BEHAVIOR Challenge: 50 full-length household tasks with 1200+ hrs of human teleoperation data</h3>
      </a>
    </div>
    <div class="text-content">
      <p>The BEHAVIOR Challenge at NeurIPS 2025 invites researchers to tackle long-horizon, everyday household tasks in realistic virtual home environments, supported by a large dataset of 10,000 richly annotated expert trajectories (over 1,200 hours) to advance robot planning and control in complex, human-centric settings.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="https://momagen-rss.github.io/">
      <img src="{{site.baseurl}}/assets/images/momagen.png" alt="momagen">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="https://momagen-rss.github.io/" class="title-link">
        <h3>MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation</h3>
      </a>
    </div>
    <div class="text-content">
      <p>MoMaGen automatically generates diverse training datasets for bimanual mobile manipulation by solving constrained optimization problems that ensure robot reachability and camera visibility from minimal human demonstrations.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="https://behavior-robot-suite.github.io/">
      <img src="{{site.baseurl}}/assets/images/brs_hardware.jpg" alt="brs">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="https://behavior-robot-suite.github.io/" class="title-link">
        <h3>BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities</h3>
      </a>
    </div>
    <div class="text-content">
      <p>We introduce the BEHAVIOR Robot Suite (BRS) for household mobile manipulation, featuring a bimanual wheeled robot with 4-DoF torso that achieves critical capabilities in coordination, navigation, and reachability. Our framework includes a cost-effective teleoperation interface and novel algorithm for learning whole-body visuomotor policies.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="https://behavior.stanford.edu">
      <img src="{{site.baseurl}}/assets/images/b1k.jpg" alt="b1k">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="https://behavior.stanford.edu" class="title-link">
        <h3>BEHAVIOR-1K: A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation</h3>
      </a>
    </div>
    <div class="text-content">
      <p>BEHAVIOR-1K is a comprehensive simulation benchmark for human-centered robotics with 1,000 real-world tasks. Powered by NVIDIA's Omniverse, it features diverse scenes, objects, and activities with realistic rendering and physics simulation. This benchmark aims to advance embodied AI and robot learning research.</p>
    </div>
  </div>
</div>

## Past Projects

<div class="container">
  <div class="image-container">
    <a href="/portfolio_manipulation/omnids">
      <img src="{{site.baseurl}}/assets/images/omnids.gif" alt="omnids">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_manipulation/omnids" class="title-link">
        <h3>Omnid Mocobots: Predicting Human Intentions for Robot Interaction</h3>
      </a>
    </div>
    <div class="text-content">
      <p>Enhancing human–robot collaboration with Omnid Mocobots, this work leverages imitation learning methods – including ACT and Diffusion Policy - to effectively forecast human intent during shared mobile manipulation tasks.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_manipulation/robot-chef">
      <img src="{{site.baseurl}}/assets/images/robot_chef.gif" alt="robot-chef">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_manipulation/robot-chef" class="title-link">
        <h3>Autonomous Robot Chef</h3>
      </a>
    </div>
    <div class="text-content">
      <p>A voice‐controlled robot cooking assistant that integrates LLM for recipe planning, CLIP for object detection, and MediaPipe with LSTM for hand gesture recognition. Users interact with a robot arm through Alexa commands to collaboratively prepare meals.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_perception_nav/slam">
      <img src="{{site.baseurl}}/assets/images/slam_gif.gif" alt="slam">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_perception_nav/slam" class="title-link">
        <h3>Feature-based EKF-SLAM from Scratch</h3>
      </a>
    </div>
    <div class="text-content">
      <p>Developed from the ground up in C++ and simulated using ROS2 and Rviz, this feature‐based EKF-SLAM system employs custom landmark detection algorithms, enabling simultaneous mapping of the environment and precise self‐localization.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_manipulation/jenga">
      <img src="{{site.baseurl}}/assets/images/jenga.gif" alt="jenga-project">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_manipulation/jenga" class="title-link">
        <h3>Jengabells - Franka Plays Jenga</h3>
      </a>
    </div>
    <div class="text-content">
      <p>By combining computer vision for tower and brick recognition, a custom MoveIt API for motion planning, and fine-tuned MobileNet for hand detection, this solution empowers a Franka Emika Panda robot arm to play Jenga with finesse.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_ML/nlp-project">
      <img src="{{site.baseurl}}/assets/images/nlp_pipeline.jpg" alt="nlp-project">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_ML/nlp-project" class="title-link">
        <h3>Edge-conditioned Graph Conlution Networks with Dependency Parsing</h3>
      </a>
    </div>
    <div class="text-content">
      <p>Merging dependency parsing with pre-trained language models through edge-conditioned graph convolutional networks, this approach attains 87.36% accuracy on IMDB binary sentiment analysis using fewer parameters than traditional methods—all while keeping sentence representations both semantic and syntactic.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_manipulation/kuka">
      <img src="{{site.baseurl}}/assets/images/449.gif" alt="kuka">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_manipulation/kuka" class="title-link">
        <h3>KUKA youBot Mobile Manipulation</h3>
      </a>
    </div>
    <div class="text-content">
      <p>Here, trajectory planning and control were implemented for a KUKA youBot mobile manipulator—a 5R arm on a four-wheeled mecanum base—to execute an eight-segment pick-and-place routine using feedforward control and odometry-based kinematics simulation for precise object transfer.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_perception_nav/visual-odom">
      <img src="{{site.baseurl}}/assets/images/visual-odom1.gif" alt="visual-odom">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_perception_nav/visual-odom" class="title-link">
        <h3>RoI-bounded Visual Odometry</h3>
      </a>
    </div>
    <div class="text-content">
      <p>Optimizing monocular visual odometry through a region-of-interest (ROI) strategy for feature detection, this solution significantly cuts computational demands while preserving the accuracy usually achieved by processing whole image frames.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_perception_nav/swarm">
      <img src="{{site.baseurl}}/assets/images/swarm-tn.png" alt="swarm">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_perception_nav/swarm" class="title-link">
        <h3>Swarm Shape and Behavior Control</h3>
      </a>
    </div>
    <div class="text-content">
      <p>Two swarm control algorithms were developed: one mimics the Brazil nut effect to spatially sort robots by size, and the other replicates Reynolds' flocking behavior to coordinate movement reminiscent of bird flocks—all using distributed control techniques.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_mechatronics/quadrotor">
      <img src="{{site.baseurl}}/assets/images/quadrotor.jpg" alt="quadrotor">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_mechatronics/quadrotor" class="title-link">
        <h3>Quadrotor Design and Control</h3>
      </a>
    </div>
    <div class="text-content">
      <p>An autonomous quadrotor drone was built using IMU integration, PID control, joystick input, and Vive lighthouse positioning, resulting in a versatile platform capable of both manual and autonomous flight.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_mechatronics/mario">
      <img src="{{site.baseurl}}/assets/images/mario-luigi.png" alt="mario">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_mechatronics/mario" class="title-link">
        <h3>Mario Motorcycle</h3>
      </a>
    </div>
    <div class="text-content">
      <p>Inspired by Mario Kart, a line-following motorcycle was built using a Raspberry Pi Pico W for image processing and a PIC microcontroller for precise steering control, complete with custom-designed PCBs — earning “Best Design” at the 2023 Northwestern Tech Cup.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_mechatronics/imu-emg">
      <img src="{{site.baseurl}}/assets/images/imu-emg.jpg" alt="imu-emg">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_mechatronics/imu-emg" class="title-link">
        <h3>IMU & EMG Controlled Robot Arm</h3>
      </a>
    </div>
    <div class="text-content">
      <p>A collaborative project where two teams created the IMUGripulator - a 2-DOF robotic arm system combining IMU-based joint control and EMG-based gripper control, built using micro:bit v2 microcontrollers and programmed in C, featuring various sensors including a 9-DOF IMU for tilt-based movement and capacitive touch controls for sensitivity adjustment.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_mechatronics/dice-cup">
      <img src="{{site.baseurl}}/assets/images/314.gif" alt="dice-cup">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_mechatronics/dice-cup" class="title-link">
        <h3>Dice in a Cup Simulation from Scratch</h3>
      </a>
    </div>
    <div class="text-content">
      <p>A physics simulation project that models a dice bouncing inside a spinning cup, implementing Lagrangian dynamics to handle the 6 degrees of freedom system, including collision detection between the dice's corners and cup edges, while accounting for gravitational and external forces to maintain the cup's position.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_web/os-env">
      <img src="{{site.baseurl}}/assets/images/os-env.jpg" alt="os-env">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_web/os-env" class="title-link">
        <h3>Orchestration Scripting Environment</h3>
      </a>
    </div>
    <div class="text-content">
      <p>Orchestration Scripts is a framework designed to foster effective work practices. It detects various workplace situations and delivers tailored strategies by abstracting organizational processes, structures, venues, and tools into computational concepts.</p>
    </div>
  </div>
</div>

<div class="container">
  <div class="image-container">
    <a href="/portfolio_ML/iExpressionNet">
      <img src="{{site.baseurl}}/assets/images/emotion_detection.png" alt="emotion-detection">
    </a>
  </div>
  <div class="text-container">
    <div class="header-row">
      <a href="/portfolio_ML/iExpressionNet" class="title-link">
        <h3>iExpressionNet</h3>
      </a>
    </div>
    <div class="text-content">
      <p>Enhancing facial expression detection for individual users, iExpressionNet employs a two-stage approach. Initially, a CNN is trained on the general FER-13 dataset and then fine-tuned using personalized expression data — with frozen convolutional layers — all integrated with OpenCV for robust face detection.</p>
    </div>
  </div>
</div>

<style>
.container {
  display: flex;
  margin-bottom: 10px;
  gap: 10px;
}

.image-container {
  flex: 0 0 200px;
  height: 100px;
  overflow: hidden;
}

.image-container img {
  width: 100%;
  height: 100%;
  object-fit: cover;
  object-position: center;
  display: block;
  transition: opacity 0.2s;
}

/* Special handling for logo-style images in the experience/education tables */
table img {
  width: 60px;
  height: 60px;
  object-fit: contain;
}

.image-container img:hover {
  opacity: 0.8;
}

.text-container {
  flex: 1;
  display: flex;
  flex-direction: column;
  min-height: 100px;
  justify-content: flex-start;
}

.header-row {
  display: flex;
  justify-content: space-between;
  align-items: flex-start;
  margin-bottom: 0.25rem;
}

.header-row h3 {
  margin: 0;
  font-size: 0.8rem;
  color: #333;
  transition: color 0.2s;
  line-height: 1.2;
}

.title-link {
  text-decoration: none;
  color: inherit;
}

.title-link:hover h3 {
  color: #0066cc;
  text-decoration: underline;
}

.text-content p {
  margin: 0;
  font-size: 0.6rem;
  line-height: 1.4;
  color: #666;
}
</style>